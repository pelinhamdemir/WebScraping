import requests
from bs4 import BeautifulSoup
import json
import time
from scholarly import scholarly

# Set up Google API key and Custom Search Engine ID
GOOGLE_API_KEY = "the_key"
CUSTOM_SEARCH_ENGINE_ID = "the_ID"

def fetch_html(url):
    """Fetches the HTML content of the webpage."""
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            return response.text
        else:
            print(f"Failed to fetch {url}, status code: {response.status_code}")
            return None
    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None

def extract_text_from_tags(soup, tags):
    """Extracts text from the specified tags."""
    extracted_text = []
    for tag in tags:
        elements = soup.find_all(tag)
        for elem in elements:
            text = elem.get_text(strip=True)
            if text:  # Only add non-empty text
                extracted_text.append(text)
    return extracted_text

def dynamic_scraping(url):
    """Dynamic scraping function."""
    html_content = fetch_html(url)
    if not html_content:
        return None

    soup = BeautifulSoup(html_content, 'html.parser')

    # Define possible tags for descriptions
    possible_tags = ['h1', 'h2', 'h3', 'p', 'span', 'div']
    extracted_text = extract_text_from_tags(soup, possible_tags)

    # Post-process to get the longest or most relevant description
    descriptions = sorted(extracted_text, key=len, reverse=True)

    # Return top-n descriptions (you can customize this)
    return descriptions[:5]  # Top 5 descriptions

def query_google_for_urls(user_input):
    """Generates a list of relevant URLs using Google Custom Search API."""
    search_url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": GOOGLE_API_KEY,
        "cx": CUSTOM_SEARCH_ENGINE_ID,
        "q": user_input,
    }

    try:
        response = requests.get(search_url, params=params)
        response.raise_for_status()
        data = response.json()

        # Extract URLs from the search results
        urls = [item["link"] for item in data.get("items", [])]
        return urls
    except requests.RequestException as e:
        print(f"Error querying Google API: {e}")
        return []
    except KeyError:
        print("Unexpected response format from Google API.")
        return []

def save_to_file(data, filename="scraped_results.json"):
    """Saves the data to a JSON file."""
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"Results saved to '{filename}'")
    except Exception as e:
        print(f"Error saving results to file: {e}")

def query_google_scholar(query):
    """Fetch academic reports using scholarly library."""
    try:
        search_results = scholarly.search_pubs(query)
        reports = []

        for result in search_results:
            # Extract relevant fields
            report = {
                "title": result['bib'].get('title', 'No title'),
                "authors": result['bib'].get('author', 'No authors'),
                "abstract": result['bib'].get('abstract', 'No abstract'),
                "year": result['bib'].get('pub_year', 'No year'),
                "url": result.get('eprint_url', 'No URL'),
            }
            reports.append(report)

            # Stop after collecting 10 results
            if len(reports) >= 10:
                break

        return reports
    except Exception as e:
        print(f"Error fetching Google Scholar data: {e}")
        return []

def save_reports_to_file(reports, filename="academic_reports.json"):
    """Save academic reports to a JSON file."""
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(reports, f, ensure_ascii=False, indent=4)
        print(f"Academic reports saved to '{filename}'")
    except Exception as e:
        print(f"Error saving academic reports to file: {e}")

def main():
    user_input = input("Enter your query: ").strip()

    if not user_input:
        print("Query cannot be empty. Please try again.")
        return

    # Fetch relevant URLs using the existing method
    print("Fetching relevant URLs...")
    urls = query_google_for_urls(user_input)

    if not urls:
        print("No URLs found. Please try again.")
        return

    # Scraping the URLs
    print("Scraping the URLs...")
    results = {}
    for url in urls:
        print(f"Scraping URL: {url}")
        descriptions = dynamic_scraping(url)
        if descriptions:
            results[url] = descriptions
        else:
            results[url] = ["No relevant content found."]

    # Save scraped data to a file
    save_to_file(results, filename="web_scraped_results.json")

    # Fetch academic reports
    print("Fetching academic reports...")
    academic_reports = query_google_scholar(user_input)
    if academic_reports:
        save_reports_to_file(academic_reports)
    else:
        print("No academic reports found.")

if __name__ == "__main__":
    main()
